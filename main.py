import asyncio
import os
import ollama  # âœ… Import Ollama
import yt_dlp  # âœ… Use yt-dlp instead of pytube
from autogen_agentchat.agents import AssistantAgent
from dotenv import load_dotenv
from moviepy.editor import VideoFileClip

# Load environment variables
load_dotenv()

# âœ… Function to interact with Ollama
def ollama_chat(messages):
    response = ollama.chat(model="llama3", messages=messages)
    return response["message"]["content"]

# âœ… Function to download & trim video
def process_video(url):
    try:
        print(f"ğŸ“¥ Downloading video from: {url}")
        
        # âœ… Use yt-dlp to download video
        ydl_opts = {
            'format': 'best',
            'outtmpl': 'temp_video.mp4',
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])

        print(f"âœ‚ï¸ Trimming video: temp_video.mp4")
        clip = VideoFileClip("temp_video.mp4").subclip(5, 15)  # Trim first 5 seconds
        output_file = "short_video.mp4"
        clip.write_videofile(output_file, codec="libx264", audio_codec="aac")

        print(f"âœ… Short video saved: {output_file}")
        return output_file
    except Exception as e:
        print(f"âŒ Error processing video: {e}")
        return None

async def main():
    # âœ… Create AssistantAgent with Ollama function
    assistant = AssistantAgent(
        name="llama_assistant",
        model_client=lambda messages: ollama_chat(messages)  # âœ… Pass function directly
    )

    # âœ… No manual input; processing video directly
    video_url = "https://www.youtube.com/watch?v=ydRy-noAv1Y"  # Example URL

    print("ğŸ”„ Processing YouTube video...")
    short_video = process_video(video_url)

    # âœ… Send message to AI assistant & get a response
    if short_video:
        message = f"âœ… Short video '{short_video}' has been successfully created!"
    else:
        message = "âŒ Failed to process video."

    print(f"ğŸ¤– AI Input: {message}")
    
    # âœ… Fix: Use correct method `.generate_oai_reply()`
    ai_response = ollama_chat([{"role": "user", "content": message}])

    # âœ… Print AI's response
    print(f"ğŸ¤– AI Response: {ai_response}")

# Run the main function within an asyncio event loop
if __name__ == "__main__":
    asyncio.run(main())
